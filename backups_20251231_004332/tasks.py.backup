"""
ðŸš€ Tareas Celery OPTIMIZADAS con cachÃ© Redis.
"""
from celery import shared_task
from django.db import transaction, connection
from .models import Movil, Consecutive
from django.core.cache import cache
import logging
from time import sleep

logger = logging.getLogger(__name__)


@shared_task(bind=True, max_retries=3, default_retry_delay=5)
def process_save_task(self, phone, operator, user_id, file, ip):
    """
    Tarea Celery para guardar un nÃºmero de telÃ©fono en la base de datos.
    AdemÃ¡s actualiza el cachÃ© Redis si es un nÃºmero nuevo.
    """
    try:
        with transaction.atomic():
            movil = Movil.objects.create(
                file=file,
                number=phone,
                operator=operator,
                user_id=user_id,
                ip=ip
            )
        
        # ðŸš€ Actualizar cachÃ© Redis con el nuevo nÃºmero
        if operator not in ['ERROR_SCRAPING', 'No existe', 'Desconocido'] and ip != 'cache':
            try:
                from .signals import add_to_phone_cache
                add_to_phone_cache(phone, operator, file)
                logger.debug(f"[process_save_task] CachÃ© actualizado: {phone} â†’ {operator}")
            except Exception as e:
                logger.warning(f"[process_save_task] Error actualizando cachÃ©: {e}")
        
        logger.info(f"[process_save_task] âœ“ Guardado: {phone} | Operador: {operator} | Fuente: {ip}")
        return {"status": "success", "phone": phone}

    except Exception as e:
        logger.error(f"[process_save_task] âœ— Error guardando {phone}: {e}")
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

    finally:
        connection.close()


@shared_task(bind=True, max_retries=3)
def update_consecutive_task(self, consecutive_id, increment_progress=True):
    """
    Tarea Celery para actualizar el progreso de un Consecutive.
    """
    try:
        with transaction.atomic():
            consecutive = Consecutive.objects.select_for_update().get(id=consecutive_id)

            if increment_progress:
                consecutive.progres += 1

            consecutive.save()

        logger.debug(f"[update_consecutive_task] âœ“ Consecutive {consecutive_id} actualizado")
        return {"status": "success", "consecutive_id": consecutive_id}

    except Consecutive.DoesNotExist:
        logger.error(f"[update_consecutive_task] âœ— Consecutive {consecutive_id} no existe")
        return {"status": "error", "message": "Consecutive not found"}

    except Exception as e:
        logger.error(f"[update_consecutive_task] âœ— Error actualizando {consecutive_id}: {e}")
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

    finally:
        connection.close()


@shared_task
def cleanup_old_tasks():
    """
    Tarea periÃ³dica para limpiar tareas antiguas (ejecutar diariamente).
    """
    from datetime import timedelta
    from django.utils import timezone

    cutoff_date = timezone.now() - timedelta(days=30)
    logger.info("[cleanup_old_tasks] Limpieza ejecutada")
    return {"status": "success"}


@shared_task(bind=True, max_retries=3, default_retry_delay=10)
def scrape_and_save_phone_task(self, phone_number, user_id, file_name, max_attempts=3):
    """
    ðŸš€ OPTIMIZADO: Tarea Celery completa con cachÃ© Redis.
    
    Flujo:
    1. Verifica cachÃ© Redis
    2. Verifica BD PostgreSQL
    3. Si no existe: hace scraping
    4. Guarda en BD y actualiza cachÃ©
    """
    from .browser import DigiPhone
    from django.contrib.auth.models import User

    try:
        # Obtener usuario
        try:
            user = User.objects.get(id=user_id)
        except User.DoesNotExist:
            logger.error(f"[scrape_and_save_phone_task] Usuario {user_id} no existe")
            return {"status": "error", "message": "User not found", "phone": phone_number}

        # ðŸš€ PASO 1: Verificar en cachÃ© Redis + BD
        from app.views import check_scraping_in_cache_and_db
        operator, source = check_scraping_in_cache_and_db(phone_number)
        
        if operator:
            logger.info(f"[scrape_and_save_phone_task] âœ“ {phone_number} encontrado en {source} â†’ {operator}")
            
            # Verificar si ya existe en BD para este archivo
            if not Movil.objects.filter(file=file_name, number=phone_number).exists():
                with transaction.atomic():
                    Movil.objects.create(
                        file=file_name,
                        number=phone_number,
                        operator=operator,
                        user=user,
                        ip=source  # 'cache' o 'database'
                    )
                logger.info(f"[scrape_and_save_phone_task] âœ… Guardado desde {source}: {phone_number} | {operator}")
            else:
                logger.info(f"[scrape_and_save_phone_task] âš  Ya existe: {phone_number} en {file_name}")
            
            return {
                "status": "success",
                "phone": phone_number,
                "operator": operator,
                "source": source,
                "attempts": 0
            }

        # ðŸš€ PASO 2: No estÃ¡ en cachÃ© ni BD â†’ Hacer scraping
        logger.info(f"[scrape_and_save_phone_task] Iniciando scraping para {phone_number}")
        
        # Verificar si ya existe en DB (por si acaso)
        if Movil.objects.filter(file=file_name, number=phone_number).exists():
            logger.info(f"[scrape_and_save_phone_task] âš  Ya existe: {phone_number}")
            return {"status": "skipped", "phone": phone_number, "reason": "duplicate"}

        # Inicializar DigiPhone
        digi_phone = DigiPhone(user=user, reprocess=False)

        if not hasattr(digi_phone, 'cookies'):
             digi_phone.cookies = None

        # Intentar obtener operador
        last_error = None

        for attempt in range(1, max_attempts + 1):
            try:
                logger.info(f"[scrape_and_save_phone_task] Intento {attempt}/{max_attempts} para {phone_number}")

                # Obtener acceso si es necesario
                if not digi_phone.cookies:
                    if not digi_phone.get_access(get_cart=False):
                        raise Exception("No se pudo obtener acceso")

                # Consultar operador
                result = digi_phone.get_phone_number(phone=phone_number)

                if result[0] == 200:
                    operator = result[1].get('name', 'Desconocido')
                    logger.info(f"[scrape_and_save_phone_task] âœ“ {phone_number} â†’ {operator}")
                    break
                elif result[0] == 404:
                    operator = "DIGI SPAIN TELECOM, S.L."
                    logger.info(f"[scrape_and_save_phone_task] âœ“ {phone_number} â†’ {operator} (404)")
                    break
                else:
                    last_error = f"Status {result[0]}"
                    logger.warning(f"[scrape_and_save_phone_task] Intento {attempt} fallÃ³: {last_error}")

                    if attempt < max_attempts:
                        sleep(2 ** attempt)

            except Exception as e:
                last_error = str(e)
                logger.error(f"[scrape_and_save_phone_task] Error intento {attempt}: {e}")

                if attempt < max_attempts:
                    sleep(2 ** attempt)

        # Si no se obtuvo operador despuÃ©s de todos los intentos
        if operator is None:
            operator = "ERROR_SCRAPING"
            logger.error(f"[scrape_and_save_phone_task] âœ— {phone_number} â†’ ERROR_SCRAPING despuÃ©s de {max_attempts} intentos (Ãºltimo error: {last_error})")

        # ðŸš€ PASO 3: Guardar en base de datos Y actualizar cachÃ©
        with transaction.atomic():
            Movil.objects.create(
                file=file_name,
                number=phone_number,
                operator=operator,
                user=user,
                ip="scraping"
            )
        
        # Actualizar cachÃ© solo si es un operador vÃ¡lido
        if operator not in ['ERROR_SCRAPING', 'No existe', 'Desconocido']:
            try:
                from .signals import add_to_phone_cache
                add_to_phone_cache(phone_number, operator, file_name)
                logger.info(f"[scrape_and_save_phone_task] CachÃ© actualizado: {phone_number} â†’ {operator}")
            except Exception as e:
                logger.warning(f"[scrape_and_save_phone_task] Error actualizando cachÃ©: {e}")

        logger.info(f"[scrape_and_save_phone_task] âœ… Guardado: {phone_number} | {operator}")

        return {
            "status": "success",
            "phone": phone_number,
            "operator": operator,
            "attempts": attempt
        }

    except Exception as e:
        logger.error(f"[scrape_and_save_phone_task] âœ— Error crÃ­tico para {phone_number}: {e}")
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

    finally:
        connection.close()


@shared_task(bind=True)
def update_consecutive_progress_task(self, consecutive_id, increment=1):
    """
    Actualiza el progreso de un Consecutive de forma atÃ³mica.
    Auto-finaliza cuando progres >= total.
    """
    try:
        from django.utils import timezone

        with transaction.atomic():
            consecutive = Consecutive.objects.select_for_update().get(id=consecutive_id)
            consecutive.progres += increment

            # Auto-finalizar si completÃ³ el total
            if consecutive.progres >= consecutive.total and consecutive.active:
                consecutive.active = False
                consecutive.finish = timezone.now()
                logger.info(f"âœ… [update_consecutive_progress] ARCHIVO COMPLETADO: {consecutive.file} ({consecutive.progres}/{consecutive.total})")

            consecutive.save()

        logger.debug(f"[update_consecutive_progress_task] Progreso: {consecutive.file} ({consecutive.progres}/{consecutive.total})")

        return {
            "status": "success",
            "consecutive_id": consecutive_id,
            "progres": consecutive.progres,
            "total": consecutive.total,
            "completed": consecutive.progres >= consecutive.total
        }

    except Consecutive.DoesNotExist:
        logger.error(f"[update_consecutive_progress_task] Consecutive {consecutive_id} no existe")
        return {"status": "error", "message": "Consecutive not found"}

    except Exception as e:
        logger.error(f"[update_consecutive_progress_task] Error: {e}")
        raise self.retry(exc=e, countdown=5, max_retries=3)

    finally:
        connection.close()


@shared_task(bind=True)
def process_file_in_batches(self, consecutive_id, batch_size=200):
    """
    ðŸš€ OPTIMIZADO: Procesa un archivo en lotes usando cachÃ© Redis.
    
    Ventajas con cachÃ©:
    - 442K nÃºmeros en memoria (Redis)
    - Consulta < 1ms por nÃºmero en cachÃ©
    - Solo hace scraping de nÃºmeros nuevos
    """
    try:
        consecutive = Consecutive.objects.get(id=consecutive_id)

        # Verificar si el archivo sigue activo
        if not consecutive.active:
            logger.info(f"[process_file_in_batches] Archivo {consecutive.file} ya no estÃ¡ activo")
            return {"status": "cancelled", "consecutive_id": consecutive_id}

        # Obtener nÃºmeros ya procesados (en memoria para evitar query N+1)
        already_processed_numbers = set(
            Movil.objects.filter(
                file=consecutive.file,
                user=consecutive.user
            ).values_list('number', flat=True)
        )

        # Cargar todos los nÃºmeros del archivo
        import pandas as pd
        file_path = f"/opt/masterfilter/media/subido/{consecutive.file}"

        # Leer archivo (ajustar segÃºn formato)
        if consecutive.file.endswith('.xlsx'):
            df = pd.read_excel(file_path)
        elif consecutive.file.endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            logger.error(f"[process_file_in_batches] Formato no soportado: {consecutive.file}")
            return {"status": "error", "message": "Unsupported format"}

        # Obtener columna de nÃºmeros (ajustar nombre de columna)
        phone_column = df.columns[0]
        all_phones = df[phone_column].astype(str).str.strip().tolist()

        # Filtrar nÃºmeros ya procesados
        pending_phones = [p for p in all_phones if p not in already_processed_numbers]

        # Procesar solo un lote
        current_batch = pending_phones[:batch_size]

        logger.info(f"[process_file_in_batches] Procesando lote de {len(current_batch)} nÃºmeros para {consecutive.file}")

        # ðŸš€ ESTADÃSTICAS DE CACHÃ‰
        cache_hits = 0
        db_hits = 0
        scraping_needed = 0

        # Encolar tareas para este lote
        for phone in current_batch:
            # ðŸš€ Verificar en cachÃ© + BD con una sola llamada optimizada
            from app.views import check_scraping_in_cache_and_db
            operator, source = check_scraping_in_cache_and_db(phone)

            if operator:
                if source == 'cache':
                    cache_hits += 1
                elif source == 'database':
                    db_hits += 1
                
                # Guardar desde cachÃ©/BD
                process_save_task.delay(
                    phone=phone,
                    operator=operator,
                    user_id=consecutive.user.id,
                    file=consecutive.file,
                    ip=source  # 'cache' o 'database'
                )

                # Incrementar progreso para nÃºmeros de cachÃ©/BD
                update_consecutive_progress_task.delay(
                    consecutive_id=consecutive.id,
                    increment=1
                )

            else:
                scraping_needed += 1
                # Scraping vÃ­a Celery
                scrape_and_save_phone_task.delay(
                    phone_number=phone,
                    user_id=consecutive.user.id,
                    file_name=consecutive.file,
                    max_attempts=3
                )

        # ðŸ“Š LOG DE ESTADÃSTICAS
        total_batch = len(current_batch)
        cache_hit_rate = (cache_hits / total_batch * 100) if total_batch > 0 else 0
        logger.info(f"""
        ðŸ“Š ESTADÃSTICAS DEL LOTE ({consecutive.file}):
        Total procesado: {total_batch}
        â””â”€ CachÃ© Redis: {cache_hits} ({cache_hit_rate:.1f}%)
        â””â”€ Base de datos: {db_hits} ({db_hits/total_batch*100 if total_batch > 0 else 0:.1f}%)
        â””â”€ Requiere scraping: {scraping_needed} ({scraping_needed/total_batch*100 if total_batch > 0 else 0:.1f}%)
        """)

        # Si quedan mÃ¡s nÃºmeros pendientes, programar el siguiente lote
        remaining = len(pending_phones) - batch_size
        if remaining > 0:
            logger.info(f"[process_file_in_batches] Quedan {remaining} nÃºmeros. Programando siguiente lote en 15s...")

            # Programar siguiente lote con delay
            process_file_in_batches.apply_async(
                args=[consecutive_id, batch_size],
                countdown=5
            )
        else:
            logger.info(f"[process_file_in_batches] âœ… Archivo {consecutive.file} completamente encolado")

        return {
            "status": "success",
            "consecutive_id": consecutive_id,
            "batch_processed": len(current_batch),
            "remaining": max(0, remaining),
            "cache_hits": cache_hits,
            "db_hits": db_hits,
            "scraping_needed": scraping_needed
        }

    except Consecutive.DoesNotExist:
        logger.error(f"[process_file_in_batches] Consecutive {consecutive_id} no existe")
        return {"status": "error", "message": "Consecutive not found"}

    except Exception as e:
        logger.error(f"[process_file_in_batches] Error: {e}")
        raise self.retry(exc=e, countdown=30, max_retries=3)

    finally:
        connection.close()
