"""
Tareas Celery OPTIMIZADAS con cach√© Redis y rotaci√≥n de proxies.

CARACTER√çSTICAS:
- Sistema de cach√© Redis para n√∫meros ya consultados
- Procesamiento en lotes para eficiencia
- Manejo robusto de errores con reintentos autom√°ticos
- M√©tricas y estad√≠sticas de progreso

Archivo: app/tasks.py (VERSI√ìN OPTIMIZADA)
"""
from celery import shared_task
from django.db import transaction, connection
from django.db.models import F
from .models import Movil, Consecutive
from django.core.cache import cache
import logging
from time import sleep

# Importaci√≥n del sistema de rotaci√≥n de proxies
from app.proxy_rotation_system import get_proxy_rotator

logger = logging.getLogger(__name__)


def get_user_queue_name(user_id):
    """
    Genera el nombre de cola para un usuario espec√≠fico.
    Cada usuario tiene su propia cola para garantizar procesamiento paralelo.
    """
    return f'user_queue_{user_id}'


def update_progress_directly(consecutive_id, increment=1):
    """
    Actualiza el progreso directamente en la BD sin usar cola de tareas.
    M√°s r√°pido para actualizaciones en tiempo real.
    """
    try:
        from django.utils import timezone
        consecutive = Consecutive.objects.get(id=consecutive_id)
        consecutive.progres += increment
        
        # Auto-completar si lleg√≥ al total
        if consecutive.progres >= consecutive.total and consecutive.active:
            consecutive.active = False
            consecutive.finish = timezone.now()
            logger.info(f"‚úÖ ARCHIVO COMPLETADO: {consecutive.file} ({consecutive.progres}/{consecutive.total})")
        
        consecutive.save()
        return True
    except Exception as e:
        logger.warning(f"[update_progress_directly] Error actualizando progreso {consecutive_id}: {e}")
        return False


@shared_task(bind=True, max_retries=3, default_retry_delay=5)
def process_save_task(self, phone, operator, user_id, file, ip):
    """
    Tarea Celery para guardar un n√∫mero de tel√©fono en la base de datos.
    Adem√°s actualiza el cach√© Redis si es un n√∫mero nuevo.
    
    Args:
        phone: N√∫mero de tel√©fono a guardar
        operator: Operador telef√≥nico encontrado
        user_id: ID del usuario que inici√≥ el proceso
        file: Nombre del archivo origen
        ip: Fuente de la informaci√≥n ('cache', 'database', 'scraping')
    """
    try:
        with transaction.atomic():
            movil = Movil.objects.create(
                file=file,
                number=phone,
                operator=operator,
                user_id=user_id,
                ip=ip
            )

        # Actualizar cach√© Redis con el nuevo n√∫mero
        if operator and operator not in ['', 'No existe', 'Desconocido'] and ip != 'cache':
            try:
                from .signals import add_to_phone_cache
                add_to_phone_cache(phone, operator, file)
                logger.debug(f"[process_save_task] Cach√© actualizado: {phone} ‚Üí {operator}")
            except Exception as e:
                logger.warning(f"[process_save_task] Error actualizando cach√©: {e}")

        logger.info(f"[process_save_task] ‚úì Guardado: {phone} | Operador: {operator} | Fuente: {ip}")
        return {"status": "success", "phone": phone}

    except Exception as e:
        logger.error(f"[process_save_task] ‚úó Error guardando {phone}: {e}")
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

    finally:
        connection.close()


@shared_task(bind=True, max_retries=3)
def update_consecutive_task(self, consecutive_id, increment_progress=True):
    """
    Tarea Celery para actualizar el progreso de un Consecutive.
    
    Args:
        consecutive_id: ID del registro Consecutive a actualizar
        increment_progress: Si True, incrementa el progreso en 1
    """
    try:
        with transaction.atomic():
            consecutive = Consecutive.objects.select_for_update().get(id=consecutive_id)

            if increment_progress:
                consecutive.progres += 1

            consecutive.save()

        logger.debug(f"[update_consecutive_task] ‚úì Consecutive {consecutive_id} actualizado")
        return {"status": "success", "consecutive_id": consecutive_id}

    except Consecutive.DoesNotExist:
        logger.error(f"[update_consecutive_task] ‚úó Consecutive {consecutive_id} no existe")
        return {"status": "error", "message": "Consecutive not found"}

    except Exception as e:
        logger.error(f"[update_consecutive_task] ‚úó Error actualizando {consecutive_id}: {e}")
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

    finally:
        connection.close()


@shared_task
def cleanup_old_tasks(self):
    """
    Tarea peri√≥dica para limpiar tareas antiguas.
    Ejecutar diariamente mediante beat scheduler.
    """
    from datetime import timedelta
    from django.utils import timezone

    cutoff_date = timezone.now() - timedelta(days=30)
    logger.info(f"[cleanup_old_tasks] Limpieza ejecutada hasta {cutoff_date}")
    return {"status": "success", "cutoff_date": str(cutoff_date)}


@shared_task
def sync_progress_with_movil():
    """
    Sincroniza el progreso de todos los archivos activos con la cantidad
    real de registros en Movil. Esto garantiza que el frontend siempre
    muestre el progreso correcto incluso si hay tareas antiguas sin consecutive_id.
    """
    from django.utils import timezone
    
    synced = []
    completed = []
    
    for c in Consecutive.objects.filter(active=True):
        count = Movil.objects.filter(file=c.file).count()
        
        if count != c.progres:
            old_progres = c.progres
            c.progres = count
            
            # Auto-completar si lleg√≥ al total
            if c.progres >= c.total:
                c.active = False
                c.finish = timezone.now()
                completed.append(f"{c.file}: {c.progres}/{c.total}")
            
            c.save()
            synced.append(f"{c.file}: {old_progres} ‚Üí {c.progres}/{c.total}")
    
    if synced:
        logger.info(f"[sync_progress] ‚úÖ Sincronizados: {len(synced)} archivos")
        for s in synced[:5]:  # Solo mostrar primeros 5
            logger.info(f"[sync_progress]   - {s}")
    
    if completed:
        for comp in completed:
            logger.info(f"[sync_progress] üéâ COMPLETADO: {comp}")
    
    return {
        "status": "success",
        "synced_count": len(synced),
        "completed_count": len(completed)
    }


@shared_task
def check_and_requeue_orphan_files():
    """
    Detecta archivos activos que no tienen tareas en cola y los re-encola.
    Esto previene que archivos queden "hu√©rfanos" sin procesar.
    """
    import redis
    from django.utils import timezone
    from datetime import timedelta
    
    r = redis.Redis(host='127.0.0.1', port=6379, db=0)
    requeued = []
    
    # Buscar archivos activos que no han avanzado en los √∫ltimos 2 minutos
    stale_threshold = timezone.now() - timedelta(minutes=2)
    
    for c in Consecutive.objects.filter(active=True, progres__lt=F('total')):
        # Verificar si hay tareas en la cola del usuario
        user_queue = f'user_queue_{c.user.id}'
        queue_count = r.llen(user_queue)
        
        # Contar registros actuales
        current_count = Movil.objects.filter(file=c.file).count()
        
        # Si el progreso no ha cambiado y no hay tareas en cola, re-encolar
        if queue_count == 0 and current_count < c.total:
            # Verificar que realmente no haya tareas pendientes
            celery_queue_count = r.llen('celery')
            
            # Solo re-encolar si el archivo est√° realmente estancado
            if current_count == c.progres or c.progres == 0:
                logger.warning(f"[check_orphan] ‚ö† Archivo hu√©rfano detectado: {c.file} (ID: {c.id})")
                logger.info(f"[check_orphan]   Usuario: {c.user.id}, Progreso: {c.progres}/{c.total}, Cola: {queue_count}")
                
                # Re-encolar el proceso
                process_file_in_batches.apply_async(
                    kwargs={
                        'consecutive_id': c.id,
                        'batch_size': 100
                    },
                    queue=user_queue
                )
                
                requeued.append(f"{c.file} (ID: {c.id})")
                logger.info(f"[check_orphan] ‚úÖ Re-encolado: {c.file}")
    
    if requeued:
        logger.info(f"[check_orphan] üîÑ Re-encolados {len(requeued)} archivos hu√©rfanos")
    
    return {
        "status": "success",
        "requeued_count": len(requeued),
        "requeued_files": requeued
    }


@shared_task(bind=True, max_retries=3, default_retry_delay=10)
def scrape_and_save_phone_task(self, phone_number, user_id, file_name, max_attempts=3, consecutive_id=None):
    """
    Tarea Celery completa para consultar y guardar un n√∫mero telef√≥nico.
    
    Flujo:
    1. Verifica cach√© Redis
    2. Verifica BD PostgreSQL
    3. Si no existe: hace scraping con DigiPhone (con reintentos y cambio de proxy)
    4. Guarda en BD y actualiza cach√©
    5. Actualiza el progreso del archivo (Consecutive)
    
    Args:
        phone_number: N√∫mero telef√≥nico a consultar
        user_id: ID del usuario
        file_name: Nombre del archivo origen
        max_attempts: N√∫mero m√°ximo de reintentos con diferentes proxies
        consecutive_id: ID del Consecutive para actualizar progreso
    """
    from django.contrib.auth.models import User

    try:
        # Obtener usuario
        try:
            user = User.objects.get(id=user_id)
        except User.DoesNotExist:
            logger.error(f"[scrape_and_save_phone_task] Usuario {user_id} no existe")
            return {"status": "error", "message": "User not found", "phone": phone_number}

        logger.info(f"[scrape_and_save_phone_task] üìû Procesando: {phone_number} | Archivo: {file_name} | Usuario: {user_id}")

        # Paso 1: Verificar en cach√© Redis + BD
        from app.views import check_scraping_in_cache_and_db
        operator, source = check_scraping_in_cache_and_db(phone_number)

        if operator:
            logger.info(f"[scrape_and_save_phone_task] ‚úì {phone_number} encontrado en {source} ‚Üí {operator}")

            # Verificar si ya existe en BD para este archivo
            if not Movil.objects.filter(file=file_name, number=phone_number).exists():
                with transaction.atomic():
                    Movil.objects.create(
                        file=file_name,
                        number=phone_number,
                        operator=operator,
                        user=user,
                        ip=source
                    )
                logger.info(f"[scrape_and_save_phone_task] ‚úÖ Guardado desde {source}: {phone_number} | {operator}")
            else:
                logger.info(f"[scrape_and_save_phone_task] ‚ö† Ya existe: {phone_number} en {file_name}")

            # Actualizar progreso del archivo directamente (sin tarea async)
            if consecutive_id:
                update_progress_directly(consecutive_id, increment=1)
                logger.info(f"[scrape_and_save_phone_task] üìä Progreso +1 directo (cache/BD) para consecutive_id={consecutive_id}")

            return {
                "status": "success",
                "phone": phone_number,
                "operator": operator,
                "source": source,
                "attempts": 0
            }

        # Paso 2: No est√° en cach√© ni BD ‚Üí Hacer scraping
        logger.info(f"[scrape_and_save_phone_task] üîç No encontrado, iniciando scraping para {phone_number}")

        # Verificar duplicado en DB (por si acaso)
        if Movil.objects.filter(file=file_name, number=phone_number).exists():
            logger.info(f"[scrape_and_save_phone_task] ‚ö† Ya existe: {phone_number}")
            # Actualizar progreso aunque sea duplicado (directo)
            if consecutive_id:
                update_progress_directly(consecutive_id, increment=1)
                logger.info(f"[scrape_and_save_phone_task] üìä Progreso +1 directo (duplicado) para consecutive_id={consecutive_id}")
            return {"status": "skipped", "phone": phone_number, "reason": "duplicate"}

        # Usar DigiPhone para scraping con reintentos
        from .browser import DigiPhone
        digi_phone = DigiPhone(user=user, reprocess=False)

        operator = None
        attempts_made = 0
        
        # Intentar con m√∫ltiples proxies
        for attempt in range(max_attempts):
            attempts_made = attempt + 1
            
            try:
                # Obtener acceso (modo simple: solo cookies)
                if not digi_phone.get_access(get_cart=False):
                    logger.warning(f"[scrape_and_save_phone_task] Intento {attempts_made}/{max_attempts}: No se pudo obtener acceso para {phone_number}, cambiando proxy...")
                    digi_phone.change_position()
                    continue

                # Consultar operador
                result = digi_phone.get_phone_number(phone=phone_number)

                if result[0] == 200:
                    operator = result[1].get('name', 'Desconocido')
                    logger.info(f"[scrape_and_save_phone_task] ‚úì {phone_number} ‚Üí {operator} (intento {attempts_made})")
                    break  # √âxito, salir del loop
                elif result[0] == 404:
                    operator = "DIGI SPAIN TELECOM, S.L."
                    logger.info(f"[scrape_and_save_phone_task] ‚úì {phone_number} ‚Üí {operator} (404 - intento {attempts_made})")
                    break  # √âxito, salir del loop
                else:
                    logger.warning(f"[scrape_and_save_phone_task] Intento {attempts_made}/{max_attempts}: Status {result[0]} para {phone_number}, cambiando proxy...")
                    digi_phone.change_position()
                    
            except Exception as e:
                logger.warning(f"[scrape_and_save_phone_task] Intento {attempts_made}/{max_attempts}: Error para {phone_number}: {str(e)[:100]}, cambiando proxy...")
                digi_phone.change_position()
        
        # Si despu√©s de todos los intentos no se obtuvo operador v√°lido
        if not operator or operator in ['', 'No existe', 'Desconocido', 'ERROR_SCRAPING']:
            logger.error(f"[scrape_and_save_phone_task] ‚úó Fall√≥ despu√©s de {max_attempts} intentos para {phone_number} - NO se guarda en BD")
            
            # S√ç actualizar progreso porque se intent√≥ procesar (directo)
            if consecutive_id:
                update_progress_directly(consecutive_id, increment=1)
                logger.info(f"[scrape_and_save_phone_task] üìä Progreso +1 directo (fallido) para consecutive_id={consecutive_id}")
            
            return {
                "status": "failed",
                "phone": phone_number,
                "reason": "No se obtuvo operador v√°lido"
            }

        # Paso 3: Guardar en base de datos SOLO si hay operador v√°lido
        with transaction.atomic():
            Movil.objects.create(
                file=file_name,
                number=phone_number,
                operator=operator,
                user=user,
                ip="scraping"
            )

        # Actualizar cach√© si fue exitoso
        try:
            from .signals import add_to_phone_cache
            add_to_phone_cache(phone_number, operator, file_name)
            logger.info(f"[scrape_and_save_phone_task] Cach√© actualizado: {phone_number} ‚Üí {operator}")
        except Exception as e:
            logger.warning(f"[scrape_and_save_phone_task] Error actualizando cach√©: {e}")

        logger.info(f"[scrape_and_save_phone_task] ‚úÖ {phone_number} ‚Üí {operator} | Archivo: {file_name} | Usuario: {user_id}")

        # Actualizar progreso del archivo (directo)
        if consecutive_id:
            update_progress_directly(consecutive_id, increment=1)
            logger.info(f"[scrape_and_save_phone_task] üìä Progreso +1 directo (√©xito) para consecutive_id={consecutive_id}")
        else:
            logger.warning(f"[scrape_and_save_phone_task] ‚ö† No hay consecutive_id, progreso NO actualizado para {phone_number}")

        return {
            "status": "success",
            "phone": phone_number,
            "operator": operator
        }

    except Exception as e:
        logger.error(f"[scrape_and_save_phone_task] ‚úó Error cr√≠tico para {phone_number}: {e}")
        # Actualizar progreso incluso en error para que el archivo no se quede atorado (directo)
        if consecutive_id:
            update_progress_directly(consecutive_id, increment=1)
            logger.info(f"[scrape_and_save_phone_task] üìä Progreso +1 directo (error) para consecutive_id={consecutive_id}")
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

    finally:
        connection.close()


@shared_task(bind=True)
def update_consecutive_progress_task(self, consecutive_id, increment=1):
    """
    Actualiza el progreso de un Consecutive de forma at√≥mica.
    Auto-finaliza cuando progres >= total.
    
    Args:
        consecutive_id: ID del registro Consecutive
        increment: Cantidad a incrementar el progreso (default: 1)
    """
    try:
        from django.utils import timezone

        with transaction.atomic():
            consecutive = Consecutive.objects.select_for_update().get(id=consecutive_id)
            consecutive.progres += increment

            # Auto-finalizar si complet√≥ el total
            if consecutive.progres >= consecutive.total and consecutive.active:
                consecutive.active = False
                consecutive.finish = timezone.now()
                logger.info(
                    f"‚úÖ [update_consecutive_progress] ARCHIVO COMPLETADO: "
                    f"{consecutive.file} ({consecutive.progres}/{consecutive.total})"
                )

            consecutive.save()

        progress_pct = (consecutive.progres / consecutive.total * 100) if consecutive.total > 0 else 0
        logger.debug(
            f"[update_consecutive_progress_task] Progreso: "
            f"{consecutive.file} ({consecutive.progres}/{consecutive.total}) - {progress_pct:.1f}%"
        )

        return {
            "status": "success",
            "consecutive_id": consecutive_id,
            "progres": consecutive.progres,
            "total": consecutive.total,
            "progress_percentage": round(progress_pct, 2),
            "completed": consecutive.progres >= consecutive.total
        }

    except Consecutive.DoesNotExist:
        logger.error(f"[update_consecutive_progress_task] Consecutive {consecutive_id} no existe")
        return {"status": "error", "message": "Consecutive not found"}

    except Exception as e:
        logger.error(f"[update_consecutive_progress_task] Error: {e}")
        raise self.retry(exc=e, countdown=5, max_retries=3)

    finally:
        connection.close()


@shared_task(bind=True)
def process_file_in_batches(self, consecutive_id, batch_size=200):
    """
    Procesa un archivo en lotes usando cach√© Redis para evitar scraping redundante.
    
    Ventajas del sistema:
    - Consulta en cach√© Redis (< 1ms por n√∫mero)
    - Solo hace scraping de n√∫meros nuevos
    - Procesamiento distribuido via Celery
    
    Args:
        consecutive_id: ID del registro Consecutive
        batch_size: Cantidad de n√∫meros a procesar por lote (default: 200)
    """
    try:
        consecutive = Consecutive.objects.get(id=consecutive_id)

        # Verificar si el archivo sigue activo
        if not consecutive.active:
            logger.info(f"[process_file_in_batches] Archivo {consecutive.file} ya no est√° activo")
            return {"status": "cancelled", "consecutive_id": consecutive_id}

        # Obtener n√∫meros ya procesados (evita query N+1)
        already_processed_numbers = set(
            Movil.objects.filter(
                file=consecutive.file,
                user=consecutive.user
            ).values_list('number', flat=True)
        )

        # Cargar todos los n√∫meros del archivo
        import pandas as pd
        file_path = f"/opt/masterfilter/media/subido/{consecutive.file}"

        # Determinar formato y leer archivo
        if consecutive.file.endswith('.xlsx'):
            df = pd.read_excel(file_path)
        elif consecutive.file.endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            logger.error(f"[process_file_in_batches] Formato no soportado: {consecutive.file}")
            return {"status": "error", "message": "Unsupported format"}

        # Obtener columna de n√∫meros (primera columna)
        phone_column = df.columns[0]
        all_phones = df[phone_column].astype(str).str.strip().tolist()

        # Filtrar n√∫meros ya procesados
        pending_phones = [p for p in all_phones if p not in already_processed_numbers]

        # Procesar solo un lote
        current_batch = pending_phones[:batch_size]

        logger.info(
            f"[process_file_in_batches] üìÅ {consecutive.file} | "
            f"Lote: {len(current_batch)} n√∫meros | "
            f"Pendientes: {len(pending_phones)}"
        )

        # M√©tricas del lote
        cache_hits = 0
        db_hits = 0
        scraping_needed = 0
        errors = 0

        # Encolar tareas para este lote
        for phone in current_batch:
            try:
                # Verificar si ya existe en BD para ESTE archivo espec√≠fico
                if Movil.objects.filter(file=consecutive.file, number=phone).exists():
                    # Ya procesado en este archivo, saltar
                    continue
                
                # Verificar en cach√© + BD con una sola llamada
                from app.views import check_scraping_in_cache_and_db
                operator, source = check_scraping_in_cache_and_db(phone)

                if operator:
                    # Encontrado en cach√© o BD - guardar directamente (sin usar tarea)
                    if source == 'cache':
                        cache_hits += 1
                    elif source == 'database':
                        db_hits += 1

                    # Guardar directamente en BD (m√°s r√°pido que encolar tarea)
                    try:
                        with transaction.atomic():
                            Movil.objects.create(
                                file=consecutive.file,
                                number=phone,
                                operator=operator,
                                user=consecutive.user,
                                ip=source
                            )
                    except Exception as save_error:
                        logger.warning(f"[process_file_in_batches] Error guardando {phone}: {save_error}")

                else:
                    # Requiere scraping - enviar a cola del usuario
                    scraping_needed += 1
                    user_queue = get_user_queue_name(consecutive.user.id)
                    scrape_and_save_phone_task.apply_async(
                        kwargs={
                            'phone_number': phone,
                            'user_id': consecutive.user.id,
                            'file_name': consecutive.file,
                            'max_attempts': 3,
                            'consecutive_id': consecutive.id
                        },
                        queue=user_queue
                    )

            except Exception as e:
                errors += 1
                logger.error(f"[process_file_in_batches] Error procesando {phone}: {e}")

        # Log de estad√≠sticas del lote
        total_batch = len(current_batch)
        cache_hit_rate = (cache_hits / total_batch * 100) if total_batch > 0 else 0
        
        logger.info(
            f"[process_file_in_batches] üìä ESTAD√çSTICAS DEL LOTE ({consecutive.file}):\n"
            f"   Total procesado: {total_batch}\n"
            f"   ‚îî‚îÄ Cach√© Redis: {cache_hits} ({cache_hit_rate:.1f}%)\n"
            f"   ‚îî‚îÄ Base de datos: {db_hits} ({db_hits/total_batch*100 if total_batch > 0 else 0:.1f}%)\n"
            f"   ‚îî‚îÄ Requiere scraping: {scraping_needed} ({scraping_needed/total_batch*100 if total_batch > 0 else 0:.1f}%)\n"
            f"   ‚îî‚îÄ Errores: {errors}"
        )

        # Programar siguiente lote si hay m√°s n√∫meros pendientes
        remaining = len(pending_phones) - batch_size
        if remaining > 0:
            logger.info(f"[process_file_in_batches] Quedan {remaining} n√∫meros. Programando siguiente lote en 1s...")

            # Enviar a la cola del usuario para mantener round-robin
            user_queue = get_user_queue_name(consecutive.user.id)
            process_file_in_batches.apply_async(
                args=[consecutive_id, batch_size],
                countdown=1,  # Reducido de 5s a 1s
                queue=user_queue
            )
        else:
            logger.info(f"[process_file_in_batches] ‚úÖ Archivo {consecutive.file} completamente encolado")

        return {
            "status": "success",
            "consecutive_id": consecutive_id,
            "batch_processed": len(current_batch),
            "remaining": max(0, remaining),
            "cache_hits": cache_hits,
            "db_hits": db_hits,
            "scraping_needed": scraping_needed,
            "errors": errors,
            "cache_hit_rate": round(cache_hit_rate, 2)
        }

    except Consecutive.DoesNotExist:
        logger.error(f"[process_file_in_batches] Consecutive {consecutive_id} no existe")
        return {"status": "error", "message": "Consecutive not found"}

    except Exception as e:
        logger.error(f"[process_file_in_batches] Error: {e}")
        raise self.retry(exc=e, countdown=30, max_retries=3)

    finally:
        connection.close()


# ============================================================================
# Tareas de Gesti√≥n de Proxies
# ============================================================================

@shared_task
def report_proxy_rotation_stats(self):
    """
    Reporta estad√≠sticas de rotaci√≥n de proxies.
    
    √ötil para monitoreo y debugging del sistema de proxies.
    """
    rotator = get_proxy_rotator()
    stats = rotator.get_stats()

    # Calcular top proxies por rendimiento
    top_proxies = []
    for proxy_id, times in rotator.metrics.items():
        if times:
            avg_time = sum(times) / len(times)
            top_proxies.append({
                'id': proxy_id,
                'avg_time': round(avg_time, 2),
                'attempts': rotator.attempt_counts.get(proxy_id, 0)
            })

    top_proxies.sort(key=lambda x: x['avg_time'])

    # Log del reporte
    logger.info("="*80)
    logger.info("üìä REPORTE DE ROTACI√ìN DE PROXIES")
    logger.info("="*80)
    logger.info(f"Total probados: {stats['total_proxies_tested']}")
    logger.info(f"En blacklist: {stats['blacklisted']}")
    logger.info(f"Proxies r√°pidos (<5s): {stats['fast_proxies']}")
    logger.info(f"Proxies lentos (>5s): {stats['slow_proxies']}")

    if top_proxies:
        logger.info("\nüèÜ TOP 10 PROXIES M√ÅS R√ÅPIDOS:")
        for idx, p in enumerate(top_proxies[:10], 1):
            logger.info(f"  {idx}. {p['id']} - {p['avg_time']}s (intentos: {p['attempts']})")

    logger.info("="*80)

    return {
        "status": "success",
        "stats": stats,
        "top_proxies": top_proxies[:10]
    }


@shared_task
def clear_proxy_blacklist(self):
    """
    Limpia la blacklist de proxies.
    
    √ötil para liberar todos los proxies blacklisted manualmente
    despu√©s de resolver problemas de conectividad.
    """
    rotator = get_proxy_rotator()
    before = len(rotator.blacklist)
    rotator.blacklist.clear()
    logger.info(f"üßπ Blacklist limpiada: {before} proxies liberados")
    return {"status": "success", "cleared": before}


@shared_task(bind=True)
def check_and_resume_stuck_processes(self):
    """
    Verifica procesos colgados y los reanuda si es necesario.
    
    Un proceso se considera colgado si:
    - Est√° activo (active=True)
    - El progreso no ha avanzado en las √∫ltimas 2 horas
    """
    from django.utils import timezone
    from datetime import timedelta

    stuck_threshold = timezone.now() - timedelta(hours=2)
    
    stuck_processes = Consecutive.objects.filter(
        active=True,
        created__lt=stuck_threshold
    )

    count = stuck_processes.count()
    if count > 0:
        logger.warning(f"[check_and_resume_stuck_processes] {count} procesos colgados detectados")
        
        for process in stuck_processes:
            logger.warning(f"  - {process.file} (ID: {process.id}, creado: {process.created})")
            
            # Reanudar proceso
            process_file_in_batches.delay(
                consecutive_id=process.id,
                batch_size=100
            )
        
        return {
            "status": "success",
            "message": f"{count} procesos colgados detectados y reagendados"
        }
    else:
        logger.info("[check_and_resume_stuck_processes] No hay procesos colgados")
        return {
            "status": "success",
            "message": "No hay procesos colgados"
        }


@shared_task
def get_system_stats(self):
    """
    Retorna estad√≠sticas generales del sistema.
    
    √ötil para dashboards de monitoreo.
    """
    from django.db.models import Count
    
    try:
        # Contadores b√°sicos
        total_moviles = Movil.objects.count()
        total_consecutives = Consecutive.objects.count()
        active_consecutives = Consecutive.objects.filter(active=True).count()
        completed_consecutives = Consecutive.objects.filter(active=False).count()
        
        # Distribuci√≥n por operador
        operator_distribution = dict(
            Movil.objects.values('operator')
            .annotate(count=Count('id'))
            .values_list('operator', 'count')
        )
        
        # N√∫meros por archivo
        files_stats = list(
            Consecutive.objects.values('file')
            .annotate(
                total=Count('id'),
                active=Count('id', filter=models.Q(active=True))
            )
        )
        
        return {
            "status": "success",
            "statistics": {
                "total_moviles": total_moviles,
                "total_consecutives": total_consecutives,
                "active_consecutives": active_consecutives,
                "completed_consecutives": completed_consecutives,
                "operator_distribution": operator_distribution
            }
        }
    
    except Exception as e:
        logger.error(f"[get_system_stats] Error: {e}")
        return {"status": "error", "message": str(e)}
